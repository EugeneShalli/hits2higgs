{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2b6b990-c6a2-4a9c-bced-52d7f77f3c77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hdbscan\n",
      "  Downloading hdbscan-0.8.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.20.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in ./acts-41.1.0/venv_acts/lib/python3.10/site-packages (from hdbscan) (1.26.4)\n",
      "Collecting scipy>=1.0 (from hdbscan)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting scikit-learn>=0.20 (from hdbscan)\n",
      "  Downloading scikit_learn-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Collecting joblib>=1.0 (from hdbscan)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in ./acts-41.1.0/venv_acts/lib/python3.10/site-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in ./acts-41.1.0/venv_acts/lib/python3.10/site-packages (from wandb) (4.3.8)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: psutil>=5.0.0 in ./acts-41.1.0/venv_acts/lib/python3.10/site-packages (from wandb) (7.0.0)\n",
      "Collecting pydantic<3 (from wandb)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: pyyaml in ./acts-41.1.0/venv_acts/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./acts-41.1.0/venv_acts/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.30.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in ./acts-41.1.0/venv_acts/lib/python3.10/site-packages (from wandb) (4.13.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3->wandb)\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3->wandb)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./acts-41.1.0/venv_acts/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./acts-41.1.0/venv_acts/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./acts-41.1.0/venv_acts/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./acts-41.1.0/venv_acts/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=0.20->hdbscan)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading hdbscan-0.8.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.20.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m134.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scikit_learn-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.30.0-py2.py3-none-any.whl (343 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading setproctitle-1.3.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Installing collected packages: typing-inspection, threadpoolctl, smmap, setproctitle, sentry-sdk, scipy, pydantic-core, protobuf, joblib, click, annotated-types, scikit-learn, pydantic, gitdb, hdbscan, gitpython, wandb\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/17\u001b[0m [wandb]m16/17\u001b[0m [wandb]hon]rn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 click-8.2.1 gitdb-4.0.12 gitpython-3.1.44 hdbscan-0.8.40 joblib-1.5.1 protobuf-6.31.1 pydantic-2.11.7 pydantic-core-2.33.2 scikit-learn-1.7.0 scipy-1.15.3 sentry-sdk-2.30.0 setproctitle-1.3.6 smmap-5.0.2 threadpoolctl-3.6.0 typing-inspection-0.4.1 wandb-0.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install hdbscan wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55a2c464-0e03-480f-8f58-6aec895a9743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c38bfa73-7924-47a8-9dfb-c5cdb9e6b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fatras_data(hits_file, particles_file, truth_file, max_num_hits, normalize=True, chunking=False):\n",
    "    '''\n",
    "    Function for reading TrackML .csv files (hits, particles, truth) and creating tensors\n",
    "    for the hits, track parameters, and particle information.\n",
    "    \n",
    "    Parameters:\n",
    "        hits_file (str): Path to the hits.csv file\n",
    "        particles_file (str): Path to the particles.csv file\n",
    "        truth_file (str): Path to the truth.csv file\n",
    "        max_num_hits (int): Maximum number of hits to pad to\n",
    "        normalize (bool): Whether to normalize features\n",
    "        chunking (bool): Whether CSVs should be read in chunks\n",
    "    '''\n",
    "    if chunking:\n",
    "        raise NotImplementedError(\"Chunking not implemented for this loader.\")\n",
    "\n",
    "    # Load CSVs\n",
    "    hits = pd.read_csv(hits_file)\n",
    "    particles = pd.read_csv(particles_file)\n",
    "    truth = pd.read_csv(truth_file)\n",
    "\n",
    "    # Merge hits with truth on hit_id\n",
    "    data = hits.merge(truth, on=\"hit_id\", how=\"left\")\n",
    "    # Merge with particle properties on particle_id\n",
    "    data = data.merge(particles[[\"particle_id\", \"px\", \"py\", \"pz\", \"q\"]], on=\"particle_id\", how=\"left\")\n",
    "\n",
    "    # Assign synthetic event_id (could be set dynamically in batch loading)\n",
    "    data[\"event_id\"] = 0\n",
    "\n",
    "    # Normalize selected columns\n",
    "    if normalize:\n",
    "        for col in [\"x\", \"y\", \"z\", \"px\", \"py\", \"pz\", \"q\"]:\n",
    "            mean = data[col].mean()\n",
    "            std = data[col].std()\n",
    "            if std != 0:\n",
    "                data[col] = (data[col] - mean) / std\n",
    "\n",
    "    # Shuffle and group by event (here, only one event typically)\n",
    "    data_grouped_by_event = data.groupby(\"event_id\")\n",
    "\n",
    "    def extract_hits_data(event_rows):\n",
    "        coords = event_rows[[\"x\", \"y\", \"z\"]].to_numpy(np.float32)\n",
    "        return np.pad(coords, [(0, max_num_hits - len(coords)), (0, 0)], \"constant\", constant_values=PAD_TOKEN)\n",
    "\n",
    "    def extract_track_params_data(event_rows):\n",
    "        params = event_rows[[\"px\", \"py\", \"pz\", \"q\"]].to_numpy(np.float32)\n",
    "        p = np.linalg.norm(params[:, :3], axis=1)\n",
    "        theta = np.arccos(params[:, 2] / p)\n",
    "        phi = np.arctan2(params[:, 1], params[:, 0])\n",
    "        return np.pad(np.column_stack([theta, np.sin(phi), np.cos(phi), params[:, 3]]),\n",
    "                      [(0, max_num_hits - len(params)), (0, 0)], \"constant\", constant_values=PAD_TOKEN)\n",
    "\n",
    "    def extract_hit_classes_data(event_rows):\n",
    "        class_data = event_rows[[\"particle_id\", \"weight\"]].to_numpy(np.float32)\n",
    "        return np.pad(class_data, [(0, max_num_hits - len(class_data)), (0, 0)], \"constant\", constant_values=PAD_TOKEN)\n",
    "\n",
    "    # Extract and pad for each event\n",
    "    hits_data = torch.tensor(np.stack(data_grouped_by_event.apply(extract_hits_data).values))\n",
    "    track_params_data = torch.tensor(np.stack(data_grouped_by_event.apply(extract_track_params_data).values))\n",
    "    hit_classes_data = torch.tensor(np.stack(data_grouped_by_event.apply(extract_hit_classes_data).values))\n",
    "\n",
    "    return hits_data, track_params_data, hit_classes_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ea87a6d-99f8-4f02-89ea-5f6b2b03baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0.0\n",
    "\n",
    "class FatrasTrackDataset(Dataset):\n",
    "    def __init__(self, base_dir, normalize=True, max_num_hits=None):\n",
    "        \"\"\"\n",
    "        base_dir: path to 'data/' folder\n",
    "        normalize: whether to normalize features\n",
    "        max_num_hits: if None, it will be determined automatically\n",
    "        \"\"\"\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.normalize = normalize\n",
    "        self.max_num_hits = max_num_hits\n",
    "        self.events = []\n",
    "\n",
    "        for label, class_name in [(1, \"signal\"), (0, \"background\")]:\n",
    "            class_dir = self.base_dir / class_name\n",
    "            print(f\"[INFO] Scanning {class_name} directory...\")\n",
    "            hit_files = list(class_dir.glob(\"*-hits.csv\"))\n",
    "            event_basenames = sorted(set(\n",
    "                f.stem.replace(\"-hits\", \"\")\n",
    "                for f in tqdm(hit_files, desc=f\"Processing {class_name}\")\n",
    "                if (class_dir / f\"{f.stem.replace('-hits', '')}-particles.csv\").exists() and\n",
    "                   (class_dir / f\"{f.stem.replace('-hits', '')}-truth.csv\").exists()\n",
    "            ))\n",
    "            self.events.extend([\n",
    "                {\"basename\": e, \"dir\": class_dir, \"label\": label}\n",
    "                for e in event_basenames\n",
    "            ])\n",
    "\n",
    "        if not self.events:\n",
    "            raise ValueError(\"No valid events found in the provided path.\")\n",
    "\n",
    "        np.random.shuffle(self.events)\n",
    "\n",
    "        if self.max_num_hits is None:\n",
    "            print(\"[INFO] Determining max_num_hits automatically...\")\n",
    "            self.max_num_hits = max(\n",
    "                len(pd.read_csv(event[\"dir\"] / f\"{event['basename']}-hits.csv\"))\n",
    "                for event in tqdm(self.events, desc=\"Counting hits\")\n",
    "            )\n",
    "            print(f\"[INFO] max_num_hits set to {self.max_num_hits}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.events)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        event = self.events[idx]\n",
    "        basename = event[\"basename\"]\n",
    "        event_dir = event[\"dir\"]\n",
    "        label = event[\"label\"]\n",
    "\n",
    "        hits = pd.read_csv(event_dir / f\"{basename}-hits.csv\")\n",
    "        particles = pd.read_csv(event_dir / f\"{basename}-particles.csv\")\n",
    "        truth = pd.read_csv(event_dir / f\"{basename}-truth.csv\")\n",
    "\n",
    "        # Merge hits → truth → particles\n",
    "        data = hits.merge(truth, on=\"hit_id\", how=\"left\")\n",
    "        data = data.merge(particles[[\"particle_id\", \"px\", \"py\", \"pz\", \"q\"]], on=\"particle_id\", how=\"left\")\n",
    "\n",
    "        if self.normalize:\n",
    "            for col in [\"x\", \"y\", \"z\", \"px\", \"py\", \"pz\", \"q\"]:\n",
    "                mean = data[col].mean()\n",
    "                std = data[col].std()\n",
    "                if std > 0:\n",
    "                    data[col] = (data[col] - mean) / std\n",
    "\n",
    "        def extract_hits(event_rows):\n",
    "            coords = event_rows[[\"x\", \"y\", \"z\"]].to_numpy(np.float32)[:self.max_num_hits]\n",
    "            return np.pad(coords, [(0, self.max_num_hits - len(coords)), (0, 0)], \"constant\", constant_values=PAD_TOKEN)\n",
    "\n",
    "        def extract_track_params(event_rows):\n",
    "            params = event_rows[[\"px\", \"py\", \"pz\", \"q\"]].to_numpy(np.float32)[:self.max_num_hits]\n",
    "            p = np.linalg.norm(params[:, :3], axis=1)\n",
    "            theta = np.arccos(np.clip(params[:, 2] / p, -1, 1))\n",
    "            phi = np.arctan2(params[:, 1], params[:, 0])\n",
    "            stacked = np.column_stack([theta, np.sin(phi), np.cos(phi), params[:, 3]])\n",
    "            return np.pad(stacked, [(0, self.max_num_hits - len(stacked)), (0, 0)], \"constant\", constant_values=PAD_TOKEN)\n",
    "\n",
    "        def extract_hit_classes(event_rows):\n",
    "            classes = event_rows[[\"particle_id\", \"weight\"]].copy()\n",
    "            classes = classes[:self.max_num_hits]\n",
    "            return np.pad(classes.to_numpy(np.float32), [(0, self.max_num_hits - len(classes)), (0, 0)], \"constant\", constant_values=PAD_TOKEN)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(extract_hits(data)),\n",
    "            torch.tensor(extract_track_params(data)),\n",
    "            torch.tensor(extract_hit_classes(data)),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a713d318-9c39-48b9-8b57-5b17f8f3096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = FatrasTrackDataset(\"train\", normalize=True, max_num_hits=130000)\n",
    "# loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "# for batch in dataloader:\n",
    "#     hits, track_params, hit_classes, labels = batch\n",
    "#     print(hits)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4d9d96-90f5-4ced-afae-283e64989073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffee992-baa2-4297-a508-73f424670b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7bb06e-6571-432c-9ab7-ac6cad2024eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
