{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc91f9-352f-4b3b-a8fa-8dce95c18289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "log = open(\"output_tml_dataset.log\", \"w\")\n",
    "sys.stdout = sys.stderr = log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13e3471a-6c4a-4c65-818e-defe0b012406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daded727-2421-4801-a64a-14ffad10c35c",
   "metadata": {},
   "source": [
    "# Add nhits and weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "613a8df1-01d4-4991-8116-ea881796bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process one event\n",
    "def process_event(base_folder, event_id, output_folder):\n",
    "    # Compose file paths\n",
    "    digitization_folder = os.path.join(base_folder, 'digitization')\n",
    "    \n",
    "    measurements_xyz_file = os.path.join(digitization_folder, f\"event{event_id}-measurements_global_xyz.csv\")\n",
    "    particles_file = os.path.join(base_folder, f\"event{event_id}-particles.csv\")\n",
    "    fatras_hits_file = os.path.join(base_folder, f\"event{event_id}-fatras_hits.csv\")\n",
    "    \n",
    "    # Check files exist\n",
    "    if not (os.path.isfile(measurements_xyz_file) and os.path.isfile(particles_file) and os.path.isfile(fatras_hits_file)):\n",
    "        print(f\"Skipping event {event_id}: required files missing.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # === HITS ===\n",
    "        measurements_xyz = pd.read_csv(measurements_xyz_file)\n",
    "        hits = pd.DataFrame({\n",
    "            'hit_id': range(1, len(measurements_xyz) + 1),\n",
    "            'x': measurements_xyz['global_x'],\n",
    "            'y': measurements_xyz['global_y'],\n",
    "            'z': measurements_xyz['global_z'],\n",
    "            'volume_id': measurements_xyz['volume'],\n",
    "            'layer_id': measurements_xyz['layer'],\n",
    "            'module_id': measurements_xyz['module']\n",
    "        })\n",
    "        hits.to_csv(os.path.join(output_folder, f\"event{event_id}-hits.csv\"), index=False)\n",
    "    \n",
    "        # === PARTICLES ===\n",
    "        particles = pd.read_csv(particles_file)\n",
    "        particles_output = pd.DataFrame({\n",
    "            'particle_id': particles['particle_id'],\n",
    "            'vx': particles['vx'],\n",
    "            'vy': particles['vy'],\n",
    "            'vz': particles['vz'],\n",
    "            'px': particles['px'],\n",
    "            'py': particles['py'],\n",
    "            'pz': particles['pz'],\n",
    "            'q': particles['q']\n",
    "        })\n",
    "    \n",
    "        # Calculate pT\n",
    "        particles_output[\"pT\"] = np.sqrt(particles_output[\"px\"]**2 + particles_output[\"py\"]**2)\n",
    "    \n",
    "        # Define pT-dependent weight function\n",
    "        def w_pT(pt):\n",
    "            if pt < 0.5:\n",
    "                return 0.2\n",
    "            elif pt < 3:\n",
    "                return 0.2 + 0.8 * (pt - 0.5) / (3 - 0.5)\n",
    "            else:\n",
    "                return 1.0\n",
    "    \n",
    "        particles_output[\"w_pT\"] = particles_output[\"pT\"].apply(w_pT)\n",
    "    \n",
    "        # === TRUTH ===\n",
    "        fatras_hits = pd.read_csv(fatras_hits_file)\n",
    "        truth_output = pd.DataFrame({\n",
    "            'hit_id': range(1, len(measurements_xyz) + 1),\n",
    "            'particle_id': fatras_hits['particle_id'],\n",
    "            'tx': fatras_hits['tx'],\n",
    "            'ty': fatras_hits['ty'],\n",
    "            'tz': fatras_hits['tz'],\n",
    "            'tpx': fatras_hits['tpx'],\n",
    "            'tpy': fatras_hits['tpy'],\n",
    "            'tpz': fatras_hits['tpz']\n",
    "        })\n",
    "    \n",
    "        # Calculate nhits\n",
    "        nhits_per_particle = (\n",
    "            truth_output.groupby('particle_id').size().reset_index(name='nhits')\n",
    "        )\n",
    "    \n",
    "        # Merge nhits into particles\n",
    "        particles_output = pd.merge(\n",
    "            particles_output,\n",
    "            nhits_per_particle,\n",
    "            on='particle_id',\n",
    "            how='left'\n",
    "        )\n",
    "    \n",
    "        particles_output['nhits'] = particles_output['nhits'].fillna(0).astype(int)\n",
    "    \n",
    "        # Calculate weights for truth\n",
    "        merged = truth_output.merge(hits, on='hit_id')\n",
    "        merged = merged.merge(particles_output[['particle_id', 'pT', 'w_pT']], on='particle_id', how='left')\n",
    "        merged['r'] = np.sqrt(merged['x']**2 + merged['y']**2 + merged['z']**2)\n",
    "        merged.sort_values(by=['particle_id', 'r'], inplace=True)\n",
    "    \n",
    "        weights_order = []\n",
    "        for pid, group in merged.groupby('particle_id'):\n",
    "            n_hits = len(group)\n",
    "            for i, row in enumerate(group.itertuples()):\n",
    "                if i == 0 or i == n_hits - 1:\n",
    "                    weights_order.append(1.0)\n",
    "                elif i in [1, n_hits - 2]:\n",
    "                    weights_order.append(0.7)\n",
    "                else:\n",
    "                    weights_order.append(0.4)\n",
    "    \n",
    "        merged['w_order'] = weights_order\n",
    "        merged['w_total'] = merged['w_order'] * merged['w_pT']\n",
    "        normalization_factor = merged['w_total'].sum()\n",
    "        merged['weight'] = merged['w_total'] / normalization_factor\n",
    "    \n",
    "        # Save updated files\n",
    "        particles_output.to_csv(os.path.join(output_folder, f\"event{event_id}-particles.csv\"), index=False)\n",
    "        merged[['hit_id', 'particle_id', 'tx', 'ty', 'tz', 'tpx', 'tpy', 'tpz', 'weight']].to_csv(\n",
    "            os.path.join(output_folder, f\"event{event_id}-truth.csv\"), index=False\n",
    "        )\n",
    "    \n",
    "        # print(f\"✅ Processed event {event_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing event {event_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdbe4bf-414a-4a05-bae3-01ae869c3f4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Just merge files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1575fdda-2607-44fe-81a1-40d5b44dda29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to process one event\n",
    "# def process_event(base_folder, event_id, output_folder):\n",
    "#     # Compose file paths\n",
    "#     digitization_folder = os.path.join(base_folder, 'digitization')\n",
    "    \n",
    "#     measurements_xyz_file = os.path.join(digitization_folder, f\"event{event_id}-measurements_global_xyz.csv\")\n",
    "#     particles_file = os.path.join(base_folder, f\"event{event_id}-particles.csv\")\n",
    "#     fatras_hits_file = os.path.join(base_folder, f\"event{event_id}-fatras_hits.csv\")\n",
    "    \n",
    "#     # Check files exist\n",
    "#     if not (os.path.isfile(measurements_xyz_file) and os.path.isfile(particles_file) and os.path.isfile(fatras_hits_file)):\n",
    "#         print(f\"Skipping event {event_id}: required files missing.\")\n",
    "#         return\n",
    "\n",
    "#     # Read data\n",
    "#     measurements_xyz = pd.read_csv(measurements_xyz_file)\n",
    "#     particles = pd.read_csv(particles_file)\n",
    "#     fatras_hits = pd.read_csv(fatras_hits_file)\n",
    "\n",
    "#     error_files = []\n",
    "    \n",
    "#     try:\n",
    "#         # === HITS ===\n",
    "#         hits = pd.DataFrame({\n",
    "#             'hit_id': range(1, len(measurements_xyz) + 1),\n",
    "#             'x': measurements_xyz['global_x'],\n",
    "#             'y': measurements_xyz['global_y'],\n",
    "#             'z': measurements_xyz['global_z'],\n",
    "#             'volume_id': measurements_xyz['volume'],\n",
    "#             'layer_id': measurements_xyz['layer'],\n",
    "#             'module_id': measurements_xyz['module']\n",
    "#         })\n",
    "#         hits.to_csv(os.path.join(output_folder, f\"event{event_id}-hits.csv\"), index=False)\n",
    "    \n",
    "#         # === PARTICLES ===\n",
    "#         particles_output = pd.DataFrame({\n",
    "#             'particle_id': particles['particle_id'],\n",
    "#             'vx': particles['vx'],\n",
    "#             'vy': particles['vy'],\n",
    "#             'vz': particles['vz'],\n",
    "#             'px': particles['px'],\n",
    "#             'py': particles['py'],\n",
    "#             'pz': particles['pz'],\n",
    "#             'q': particles['q']\n",
    "#         })\n",
    "#         particles_output.to_csv(os.path.join(output_folder, f\"event{event_id}-particles.csv\"), index=False)\n",
    "    \n",
    "#         # === TRUTH ===\n",
    "#         truth_output = pd.DataFrame({\n",
    "#             'hit_id': range(1, len(measurements_xyz) + 1),\n",
    "#             'particle_id': fatras_hits['particle_id'],\n",
    "#             'tx': fatras_hits['tx'],\n",
    "#             'ty': fatras_hits['ty'],\n",
    "#             'tz': fatras_hits['tz'],\n",
    "#             'tpx': fatras_hits['tpx'],\n",
    "#             'tpy': fatras_hits['tpy'],\n",
    "#             'tpz': fatras_hits['tpz']\n",
    "#         })\n",
    "        \n",
    "#         truth_output.to_csv(os.path.join(output_folder, f\"event{event_id}-truth.csv\"), index=False)\n",
    "\n",
    "#     except:\n",
    "#         print(event_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ab4652-622a-4c9f-99ae-76c3d2563d09",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Run processing ALL SIMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04c68f72-8bd7-4466-ac7c-ec04ccae4185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base folders\n",
    "base_folders = ['data/ttbar_H_production_p50/csv/background', 'data/ttbar_H_production_p50/csv/signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f0262a4-0d3f-478b-9ff2-2b184e714c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447424a299b44c06bd2a2aa980c875c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed event 000008690\n",
      "✅ Processed event 000006952\n",
      "✅ Processed event 000004561\n",
      "✅ Processed event 000004270\n",
      "✅ Processed event 000007111\n",
      "✅ Processed event 000018490\n",
      "✅ Processed event 000001438\n",
      "✅ Processed event 000008342\n",
      "✅ Processed event 000003781\n",
      "✅ Processed event 000017480\n",
      "✅ Processed event 000001711\n",
      "✅ Processed event 000008895\n",
      "✅ Processed event 000011863\n",
      "✅ Processed event 000012041\n",
      "✅ Processed event 000019976\n",
      "✅ Processed event 000011106\n",
      "✅ Processed event 000009363\n",
      "✅ Processed event 000010814\n",
      "✅ Processed event 000014125\n",
      "✅ Processed event 000013905\n",
      "✅ Processed event 000004104\n",
      "✅ Processed event 000006264\n",
      "✅ Processed event 000005976\n",
      "✅ Processed event 000006572\n",
      "✅ Processed event 000008783\n",
      "✅ Processed event 000017442\n",
      "✅ Processed event 000004895\n",
      "✅ Processed event 000008115\n",
      "✅ Processed event 000011525\n",
      "✅ Processed event 000015968\n",
      "✅ Processed event 000009956\n",
      "✅ Processed event 000018172\n",
      "✅ Processed event 000010411\n",
      "✅ Processed event 000008220\n",
      "✅ Processed event 000014054\n",
      "✅ Processed event 000003901\n",
      "❌ Error processing event 000000103: array length 19584 does not match index length 19585\n",
      "✅ Processed event 000002621\n",
      "✅ Processed event 000010746\n",
      "✅ Processed event 000003039\n",
      "✅ Processed event 000016916\n",
      "✅ Processed event 000008850\n",
      "✅ Processed event 000010386\n",
      "✅ Processed event 000003814\n",
      "✅ Processed event 000006147\n",
      "✅ Processed event 000007687\n",
      "✅ Processed event 000013378\n",
      "✅ Processed event 000003240\n",
      "✅ Processed event 000009868\n",
      "✅ Processed event 000004855\n",
      "✅ Processed event 000014195\n",
      "✅ Processed event 000009892\n",
      "✅ Processed event 000007963\n",
      "✅ Processed event 000011139\n",
      "✅ Processed event 000001006\n",
      "✅ Processed event 000001417\n",
      "✅ Processed event 000014144\n",
      "✅ Processed event 000007590\n",
      "✅ Processed event 000015543\n",
      "✅ Processed event 000017285\n",
      "✅ Processed event 000010651\n",
      "✅ Processed event 000010312\n",
      "❌ Error processing event 000008997: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m match:\n\u001b[1;32m     14\u001b[0m     event_id \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mprocess_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeasurements_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: could not extract event ID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 93\u001b[0m, in \u001b[0;36mprocess_event\u001b[0;34m(base_folder, event_id, output_folder)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pid, group \u001b[38;5;129;01min\u001b[39;00m merged\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparticle_id\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     92\u001b[0m     n_hits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(group)\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitertuples\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m n_hits \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     95\u001b[0m             weights_order\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m/app/acts-41.1.0/venv_acts/lib/python3.10/site-packages/pandas/core/frame.py:1635\u001b[0m, in \u001b[0;36mDataFrame.itertuples\u001b[0;34m(self, index, name)\u001b[0m\n\u001b[1;32m   1630\u001b[0m arrays\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[:, k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)))\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;66;03m# https://github.com/python/mypy/issues/9046\u001b[39;00m\n\u001b[1;32m   1634\u001b[0m     \u001b[38;5;66;03m# error: namedtuple() expects a string literal as the first argument\u001b[39;00m\n\u001b[0;32m-> 1635\u001b[0m     itertuple \u001b[38;5;241m=\u001b[39m \u001b[43mcollections\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamedtuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[misc]\u001b[39;49;00m\n\u001b[1;32m   1636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(itertuple\u001b[38;5;241m.\u001b[39m_make, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39marrays))\n\u001b[1;32m   1640\u001b[0m \u001b[38;5;66;03m# fallback to regular tuples\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/collections/__init__.py:414\u001b[0m, in \u001b[0;36mnamedtuple\u001b[0;34m(typename, field_names, rename, defaults, module)\u001b[0m\n\u001b[1;32m    408\u001b[0m namespace \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_tuple_new\u001b[39m\u001b[38;5;124m'\u001b[39m: tuple_new,\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__builtins__\u001b[39m\u001b[38;5;124m'\u001b[39m: {},\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamedtuple_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    412\u001b[0m }\n\u001b[1;32m    413\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda _cls, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: _tuple_new(_cls, (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m))\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;21m__new__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;21m__new__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__new__\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;21m__new__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreate new instance of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Process all events in each base folder\n",
    "for base_folder in base_folders:\n",
    "    digitization_folder = os.path.join(base_folder, 'digitization')\n",
    "    output_folder = os.path.join(base_folder, 'tml_dataset')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # List all events by finding files ending with -measurements_global_xyz.csv\n",
    "    measurements_files = glob.glob(os.path.join(digitization_folder, '*-measurements_global_xyz.csv'))\n",
    "\n",
    "    for measurements_file in tqdm(measurements_files):\n",
    "        # Extract event ID using regex\n",
    "        match = re.search(r'event(\\d+)-measurements_global_xyz.csv', os.path.basename(measurements_file))\n",
    "        if match:\n",
    "            event_id = match.group(1)\n",
    "            process_event(base_folder, event_id, output_folder)\n",
    "        else:\n",
    "            print(f\"Skipping file {measurements_file}: could not extract event ID\")\n",
    "\n",
    "print(\"Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c961046-c5ff-4a04-b006-1f7afa48a6cf",
   "metadata": {},
   "source": [
    "# Run Processing Start from Last file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85b9f7ae-72ea-4c33-9150-36fc693d7b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base folders\n",
    "base_folders = ['data/ttbar_H_production_p50/csv/background', 'data/ttbar_H_production_p50/csv/signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43baf5cf-621d-4b1a-aa63-616b395aa0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ede5de034a47dc86715f0cb80a5504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing event 000000005: array length 27632 does not match index length 27633\n",
      "❌ Error processing event 000000020: array length 26170 does not match index length 26171\n",
      "❌ Error processing event 000000027: array length 24411 does not match index length 24412\n",
      "❌ Error processing event 000000030: array length 23275 does not match index length 23276\n",
      "❌ Error processing event 000000030: array length 26757 does not match index length 26758\n",
      "❌ Error processing event 000000031: array length 28018 does not match index length 28019\n",
      "❌ Error processing event 000000034: array length 24135 does not match index length 24136\n",
      "❌ Error processing event 000000054: array length 31664 does not match index length 31665\n",
      "❌ Error processing event 000000078: array length 27183 does not match index length 27184\n",
      "❌ Error processing event 000000093: array length 23388 does not match index length 23389\n",
      "❌ Error processing event 000000103: array length 19584 does not match index length 19585\n",
      "❌ Error processing event 000000107: array length 31787 does not match index length 31788\n",
      "❌ Error processing event 000000138: array length 28905 does not match index length 28906\n",
      "❌ Error processing event 000000165: array length 32099 does not match index length 32100\n",
      "❌ Error processing event 000000166: array length 23865 does not match index length 23866\n",
      "❌ Error processing event 000000174: array length 26744 does not match index length 26745\n",
      "❌ Error processing event 000000265: array length 21228 does not match index length 21229\n",
      "❌ Error processing event 000000289: array length 31580 does not match index length 31581\n",
      "❌ Error processing event 000000308: array length 22973 does not match index length 22974\n",
      "❌ Error processing event 000000374: array length 24883 does not match index length 24884\n",
      "❌ Error processing event 000000377: array length 21414 does not match index length 21415\n",
      "❌ Error processing event 000000389: array length 28885 does not match index length 28886\n",
      "❌ Error processing event 000000402: array length 30242 does not match index length 30243\n",
      "❌ Error processing event 000000420: array length 34272 does not match index length 34273\n",
      "❌ Error processing event 000000542: array length 17671 does not match index length 17672\n",
      "❌ Error processing event 000000553: array length 26884 does not match index length 26885\n",
      "❌ Error processing event 000000638: array length 30261 does not match index length 30262\n",
      "❌ Error processing event 000000661: array length 30772 does not match index length 30773\n",
      "❌ Error processing event 000000670: array length 22961 does not match index length 22962\n",
      "❌ Error processing event 000000671: array length 34163 does not match index length 34164\n",
      "❌ Error processing event 000000687: array length 28929 does not match index length 28930\n",
      "❌ Error processing event 000000712: array length 30229 does not match index length 30230\n",
      "❌ Error processing event 000000713: array length 30018 does not match index length 30019\n",
      "❌ Error processing event 000000733: array length 24265 does not match index length 24266\n",
      "❌ Error processing event 000000769: array length 26553 does not match index length 26554\n",
      "❌ Error processing event 000000802: array length 27968 does not match index length 27969\n",
      "❌ Error processing event 000000828: array length 33738 does not match index length 33739\n",
      "❌ Error processing event 000000853: array length 33753 does not match index length 33754\n",
      "❌ Error processing event 000000888: array length 25225 does not match index length 25226\n",
      "❌ Error processing event 000000890: array length 29697 does not match index length 29698\n",
      "❌ Error processing event 000000903: array length 26815 does not match index length 26816\n",
      "❌ Error processing event 000000917: array length 23954 does not match index length 23955\n",
      "❌ Error processing event 000000935: array length 26725 does not match index length 26726\n",
      "❌ Error processing event 000000942: array length 25727 does not match index length 25728\n",
      "❌ Error processing event 000000996: array length 27183 does not match index length 27184\n",
      "❌ Error processing event 000001000: array length 26416 does not match index length 26417\n",
      "❌ Error processing event 000001019: array length 26741 does not match index length 26742\n",
      "❌ Error processing event 000001062: array length 36255 does not match index length 36256\n",
      "❌ Error processing event 000001081: array length 19108 does not match index length 19109\n",
      "❌ Error processing event 000001084: array length 24776 does not match index length 24777\n",
      "❌ Error processing event 000001087: array length 23790 does not match index length 23791\n",
      "❌ Error processing event 000001182: array length 35236 does not match index length 35237\n",
      "❌ Error processing event 000001185: array length 27132 does not match index length 27133\n",
      "❌ Error processing event 000001193: array length 24270 does not match index length 24271\n",
      "❌ Error processing event 000001197: array length 25947 does not match index length 25948\n",
      "❌ Error processing event 000001215: array length 22060 does not match index length 22061\n",
      "❌ Error processing event 000001219: array length 24424 does not match index length 24425\n",
      "❌ Error processing event 000001221: array length 19836 does not match index length 19837\n",
      "❌ Error processing event 000001227: array length 29953 does not match index length 29954\n",
      "❌ Error processing event 000001253: array length 23715 does not match index length 23716\n",
      "❌ Error processing event 000001280: array length 21385 does not match index length 21386\n",
      "❌ Error processing event 000001333: array length 23760 does not match index length 23761\n",
      "❌ Error processing event 000001366: array length 31077 does not match index length 31078\n",
      "❌ Error processing event 000001391: array length 25373 does not match index length 25374\n",
      "❌ Error processing event 000001436: array length 26318 does not match index length 26319\n",
      "❌ Error processing event 000001491: array length 21067 does not match index length 21068\n",
      "❌ Error processing event 000001510: array length 24752 does not match index length 24753\n",
      "❌ Error processing event 000001512: array length 29421 does not match index length 29422\n",
      "❌ Error processing event 000001527: array length 27363 does not match index length 27364\n",
      "❌ Error processing event 000001543: array length 21710 does not match index length 21711\n",
      "❌ Error processing event 000001562: array length 23121 does not match index length 23122\n",
      "❌ Error processing event 000001590: array length 24860 does not match index length 24861\n",
      "❌ Error processing event 000001610: array length 28626 does not match index length 28627\n",
      "❌ Error processing event 000001616: array length 24948 does not match index length 24949\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m base_folder, event_id \u001b[38;5;129;01min\u001b[39;00m tqdm(interleaved_events):\n\u001b[1;32m     34\u001b[0m     output_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtml_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mprocess_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[33], line 93\u001b[0m, in \u001b[0;36mprocess_event\u001b[0;34m(base_folder, event_id, output_folder)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pid, group \u001b[38;5;129;01min\u001b[39;00m merged\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparticle_id\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     92\u001b[0m     n_hits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(group)\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitertuples\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m n_hits \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     95\u001b[0m             weights_order\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m/app/acts-41.1.0/venv_acts/lib/python3.10/site-packages/pandas/core/frame.py:1630\u001b[0m, in \u001b[0;36mDataFrame.itertuples\u001b[0;34m(self, index, name)\u001b[0m\n\u001b[1;32m   1627\u001b[0m     fields\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;66;03m# use integer indexing because of possible duplicate column names\u001b[39;00m\n\u001b[0;32m-> 1630\u001b[0m \u001b[43marrays\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;66;03m# https://github.com/python/mypy/issues/9046\u001b[39;00m\n\u001b[1;32m   1634\u001b[0m     \u001b[38;5;66;03m# error: namedtuple() expects a string literal as the first argument\u001b[39;00m\n\u001b[1;32m   1635\u001b[0m     itertuple \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1636\u001b[0m         name, fields, rename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1637\u001b[0m     )\n",
      "File \u001b[0;32m/app/acts-41.1.0/venv_acts/lib/python3.10/site-packages/pandas/core/frame.py:1630\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1627\u001b[0m     fields\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;66;03m# use integer indexing because of possible duplicate column names\u001b[39;00m\n\u001b[0;32m-> 1630\u001b[0m arrays\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)))\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;66;03m# https://github.com/python/mypy/issues/9046\u001b[39;00m\n\u001b[1;32m   1634\u001b[0m     \u001b[38;5;66;03m# error: namedtuple() expects a string literal as the first argument\u001b[39;00m\n\u001b[1;32m   1635\u001b[0m     itertuple \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1636\u001b[0m         name, fields, rename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1637\u001b[0m     )\n",
      "File \u001b[0;32m/app/acts-41.1.0/venv_acts/lib/python3.10/site-packages/pandas/core/indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/app/acts-41.1.0/venv_acts/lib/python3.10/site-packages/pandas/core/indexing.py:1692\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1690\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tuple_indexer(tup)\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[0;32m-> 1692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple_same_dim(tup)\n",
      "File \u001b[0;32m/app/acts-41.1.0/venv_acts/lib/python3.10/site-packages/pandas/core/indexing.py:1065\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tup):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_label_like(key):\n\u001b[1;32m   1063\u001b[0m         \u001b[38;5;66;03m# We don't need to check for tuples here because those are\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m         \u001b[38;5;66;03m#  caught by the _is_nested_tuple_indexer check above.\u001b[39;00m\n\u001b[0;32m-> 1065\u001b[0m         section \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# We should never have a scalar section here, because\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m#  _getitem_lowerdim is only called after a check for\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m#  is_scalar_access, which that would be.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m section\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m   1071\u001b[0m             \u001b[38;5;66;03m# we're in the middle of slicing through a MultiIndex\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m             \u001b[38;5;66;03m# revise the key wrt to `section` by inserting an _NS\u001b[39;00m\n",
      "File \u001b[0;32m/app/acts-41.1.0/venv_acts/lib/python3.10/site-packages/pandas/core/indexing.py:1719\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1716\u001b[0m         \u001b[38;5;66;03m# re-raise with different error message, e.g. test_getitem_ndarray_3d\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositional indexers are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_getitem_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, axis: AxisInt):\n\u001b[1;32m   1720\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mEllipsis\u001b[39m:\n\u001b[1;32m   1721\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Collect all events (for each base folder)\n",
    "event_lists = {}\n",
    "for base_folder in base_folders:\n",
    "    digitization_folder = os.path.join(base_folder, 'digitization')\n",
    "    output_folder = os.path.join(base_folder, 'tml_dataset')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    measurements_files = glob.glob(os.path.join(digitization_folder, '*-measurements_global_xyz.csv'))\n",
    "    event_ids = []\n",
    "    for measurements_file in measurements_files:\n",
    "        match = re.search(r'event(\\d+)-measurements_global_xyz.csv', os.path.basename(measurements_file))\n",
    "        if match:\n",
    "            event_id = match.group(1)\n",
    "            output_file = os.path.join(output_folder, f\"event{event_id}-hits.csv\")\n",
    "            if not os.path.exists(output_file):\n",
    "                event_ids.append(event_id)\n",
    "    event_ids.sort(key=lambda x: int(x))\n",
    "    event_lists[base_folder] = event_ids\n",
    "\n",
    "# Interleave signal and background events\n",
    "bg_events = event_lists[base_folders[0]]\n",
    "sig_events = event_lists[base_folders[1]]\n",
    "max_len = max(len(bg_events), len(sig_events))\n",
    "\n",
    "interleaved_events = []\n",
    "for i in range(max_len):\n",
    "    if i < len(bg_events):\n",
    "        interleaved_events.append((base_folders[0], bg_events[i]))\n",
    "    if i < len(sig_events):\n",
    "        interleaved_events.append((base_folders[1], sig_events[i]))\n",
    "\n",
    "# Process interleaved events\n",
    "for base_folder, event_id in tqdm(interleaved_events):\n",
    "    output_folder = os.path.join(base_folder, 'tml_dataset')\n",
    "    process_event(base_folder, event_id, output_folder)\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
